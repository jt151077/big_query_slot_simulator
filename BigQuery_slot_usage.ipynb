{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTmLBxDxBAZL"
      },
      "source": [
        "### Authenticate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SeTJb51SKs_W",
        "outputId": "cf4fecbc-a9a4-4849-dbca-bc1e562c2676"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authenticated\n"
          ]
        }
      ],
      "source": [
        "# authenticate the logged in user in Colab towards GCP\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "print('Authenticated')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Find project ids using gcloud and parse the result table into an array of ids\n",
        "projects_list = !gcloud projects list\n",
        "projects_list.pop(0)\n",
        "#project_ids = ['PROJ-A','PROJ-B','...']\n",
        "project_ids = []\n",
        "\n",
        "if len(project_ids) == 0:\n",
        "  for project in projects_list:\n",
        "    a = project.replace(\" \", \",\")\n",
        "    b = a.split(\",\")\n",
        "    project_ids.append(b[0])\n",
        "\n",
        "print(project_ids)"
      ],
      "metadata": {
        "id": "QQyJuonAiVDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3umaMaIvpCp8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# declare variables for the destination tables where usage will be persisted\n",
        "project_id = 'PROJECT_ID' #project id where temporary table is written\n",
        "dataset_id = 'DATASET_ID' #dataset where temporary table is created\n",
        "table_id = 'slot_usage'\n",
        "start_date = '2024-01-01' #start date for collecting BigQuery usage\n",
        "region = 'REGION' #region such as region-eu, region-us, europe-west1 where the jobs have taken place\n",
        "\n",
        "import pandas as pd\n",
        "import pandas_gbq\n",
        "from google.cloud import bigquery\n",
        "\n",
        "df = pd.DataFrame()\n",
        "\n",
        "# loop through all project ids to retrieve usage from INFORMATION_SCHEMA table (source: https://docs.google.com/document/d/1mUFlH-seavSpPob1K9O7yz3SJ_MElI6zVt5yqyMJRco/edit?resourcekey=0-GYbZ5Tg1UD67KzkTHaMqtw#heading=h.vklkfaoltjc7)\n",
        "for pid in project_ids:\n",
        "  try:\n",
        "    client = bigquery.Client(project=pid)\n",
        "    query = f\"\"\"\n",
        "WITH\n",
        "  pay_as_go_total AS (\n",
        "  SELECT\n",
        "    SUM(total_bytes_billed) AS total_bytes_billed\n",
        "  FROM\n",
        "    `{region}`.INFORMATION_SCHEMA.JOBS_TIMELINE_BY_PROJECT\n",
        "  WHERE\n",
        "    DATE(period_start,'Europe/Oslo') > CAST('{start_date}' AS date)\n",
        "    AND (statement_type != 'SCRIPT'\n",
        "      OR statement_type IS NULL)\n",
        "    AND period_slot_ms > 0\n",
        "    AND job_type = 'QUERY'\n",
        "    AND state = 'DONE' ),\n",
        "  snapshot_data AS (\n",
        "  SELECT\n",
        "    project_id,\n",
        "    period_start,\n",
        "    period_slot_ms\n",
        "  FROM\n",
        "    `{region}`.INFORMATION_SCHEMA.JOBS_TIMELINE_BY_PROJECT\n",
        "  WHERE\n",
        "    DATE(period_start,'Europe/Oslo') > CAST('{start_date}' AS date)\n",
        "    AND (statement_type != 'SCRIPT'\n",
        "      OR statement_type IS NULL)\n",
        "    AND period_slot_ms > 0\n",
        "    AND job_type = 'QUERY' ),\n",
        "  snapshot_data_with_bytes AS (\n",
        "  SELECT\n",
        "    *\n",
        "  FROM\n",
        "    snapshot_data\n",
        "  JOIN\n",
        "    pay_as_go_total\n",
        "  ON\n",
        "    TRUE ),\n",
        "  data_by_time AS (\n",
        "  SELECT\n",
        "    project_id,\n",
        "    TIMESTAMP_TRUNC(period_start, SECOND ) AS usage_time_sec,\n",
        "    IFNULL(SUM(period_slot_ms) / 1000, 0) AS usage_slot_sec,\n",
        "    total_bytes_billed\n",
        "  FROM\n",
        "    snapshot_data_with_bytes\n",
        "  GROUP BY\n",
        "    usage_time_sec,\n",
        "    project_id,\n",
        "    total_bytes_billed),\n",
        "  max_per_minute_data_by_time AS (\n",
        "  SELECT\n",
        "    project_id,\n",
        "    TIMESTAMP_TRUNC(usage_time_sec, MINUTE ) AS usage_time,\n",
        "    MAX(usage_slot_sec) AS max_slot_per_minute,\n",
        "    total_bytes_billed\n",
        "  FROM\n",
        "    data_by_time\n",
        "  GROUP BY\n",
        "    usage_time,\n",
        "    project_id,\n",
        "    total_bytes_billed ),\n",
        "  max_per_minute_data_by_time_with_zeros AS (\n",
        "  SELECT\n",
        "    project_id,\n",
        "    usage_time,\n",
        "    IFNULL(slot_usage.max_slot_per_minute,0) AS max_slot_per_minute,\n",
        "    total_bytes_billed\n",
        "  FROM\n",
        "    UNNEST(GENERATE_TIMESTAMP_ARRAY('{start_date}', CURRENT_TIMESTAMP(), INTERVAL 1 MINUTE)) AS usage_time\n",
        "  LEFT JOIN (\n",
        "    SELECT\n",
        "      *\n",
        "    FROM\n",
        "      max_per_minute_data_by_time ) AS slot_usage\n",
        "  USING\n",
        "    (usage_time))\n",
        "SELECT\n",
        "  *\n",
        "FROM\n",
        "  max_per_minute_data_by_time_with_zeros\n",
        "ORDER BY\n",
        "  usage_time asc\n",
        "\"\"\"\n",
        "    dfx = client.query(query).to_dataframe()\n",
        "    df = pd.concat([df, dfx])\n",
        "  except:\n",
        "    print(f\"An error occurred with {pid}\")\n",
        "\n",
        "# write back the result to BigQuery: {project_id}.{dataset_id}.{table_id}'\n",
        "pandas_gbq.to_gbq(df, f'{project_id}.{dataset_id}.{table_id}', project_id=project_id, if_exists='replace')\n",
        "print('Sample table format below:')\n",
        "df.head(5)"
      ],
      "metadata": {
        "id": "AK-teRnJzVqn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define variables to match the current region ()\n",
        "earliest_date = ''\n",
        "if earliest_date == '':\n",
        "  earliest_date = df['usage_time'].min()\n",
        "\n",
        "latest_date = ''\n",
        "if latest_date == '':\n",
        "  latest_date = df['usage_time'].max()\n",
        "\n",
        "committed_slots = 100\n",
        "std_ed_price_per_slot = 0.044\n",
        "ent_ed_price_per_slot = 0.066\n",
        "ent_ed_1yc_per_slot = 0.0528\n",
        "ent_ed_3yc_per_slot = 0.0396\n",
        "ent_edplus_price_per_slot = 0.11\n",
        "ent_edplus_1yc_per_slot = 0.088\n",
        "ent_edplus_3yc_per_slot = 0.066\n",
        "\n",
        "# calculate costs per edition and commit duration, as well as pay-as-you-go\n",
        "# pricing data is extracted from https://cloud.google.com/bigquery/pricing#capacity_compute_analysis_pricing\n",
        "# Note: with BigQuery editions, slots scale by multiples of 100 (https://cloud.google.com/bigquery/docs/slots-autoscaling-intro#use_autoscaling_reservations)\n",
        "client = bigquery.Client(project=project_id)\n",
        "query = f\"\"\"\n",
        "DECLARE\n",
        "  committed_slots INT64 DEFAULT {committed_slots};\n",
        "DECLARE\n",
        "  std_ed_price_per_slot FLOAT64 DEFAULT ({std_ed_price_per_slot}/60);\n",
        "DECLARE\n",
        "  ent_ed_price_per_slot FLOAT64 DEFAULT ({ent_ed_price_per_slot}/60);\n",
        "DECLARE\n",
        "  ent_ed_1yc_per_slot FLOAT64 DEFAULT ({ent_ed_1yc_per_slot}/60);\n",
        "DECLARE\n",
        "  ent_ed_3yc_per_slot FLOAT64 DEFAULT ({ent_ed_3yc_per_slot}/60);\n",
        "DECLARE\n",
        "  ent_edplus_price_per_slot FLOAT64 DEFAULT ({ent_edplus_price_per_slot}/60);\n",
        "DECLARE\n",
        "  ent_edplus_1yc_per_slot FLOAT64 DEFAULT ({ent_edplus_1yc_per_slot}/60);\n",
        "DECLARE\n",
        "  ent_edplus_3yc_per_slot FLOAT64 DEFAULT ({ent_edplus_3yc_per_slot}/60);\n",
        "\n",
        "WITH\n",
        "  pay_as_go_tot as (\n",
        "    SELECT round(sum(distinct(total_bytes_billed))/1e+12, 2) as tot_bytes_billed_TB FROM `{project_id}.{dataset_id}.{table_id}` where usage_time > '{earliest_date}' and usage_time < '{latest_date}' AND project_id IS NOT NULL\n",
        "  ),\n",
        "  concat_slots AS (\n",
        "  SELECT\n",
        "    usage_time,\n",
        "    SUM((floor((ceiling(max_slot_per_minute) + 99) / 100) * 100)) AS tot_max_slot_per_minute,\n",
        "    SUM(max_slot_per_minute) AS sum_max_slot_per_minute\n",
        "  FROM\n",
        "    `{project_id}.{dataset_id}.{table_id}`\n",
        "  WHERE\n",
        "    usage_time > '{earliest_date}' and usage_time < '{latest_date}'\n",
        "    AND project_id IS NOT NULL\n",
        "  GROUP BY\n",
        "    usage_time),\n",
        "    price_per_min_bucket as (\n",
        "SELECT\n",
        "  *,\n",
        "  tot_max_slot_per_minute*std_ed_price_per_slot AS cost_std_ed_price_per_slot,\n",
        "  tot_max_slot_per_minute*ent_ed_price_per_slot AS cost_ent_ed_price_per_slot,\n",
        "  CASE\n",
        "    WHEN tot_max_slot_per_minute > committed_slots THEN ((tot_max_slot_per_minute-committed_slots)*ent_ed_price_per_slot)+(committed_slots*ent_ed_1yc_per_slot)\n",
        "    WHEN tot_max_slot_per_minute <= committed_slots THEN (committed_slots*ent_ed_1yc_per_slot)\n",
        "  END\n",
        "  AS cost_ent_ed_1yc_per_slot,\n",
        "  CASE\n",
        "    WHEN tot_max_slot_per_minute > committed_slots THEN ((tot_max_slot_per_minute-committed_slots)*ent_ed_price_per_slot)+(committed_slots*ent_ed_3yc_per_slot)\n",
        "    WHEN tot_max_slot_per_minute <= committed_slots THEN (committed_slots*ent_ed_3yc_per_slot)\n",
        "  END\n",
        "  AS cost_ent_ed_3yc_per_slot,\n",
        "  tot_max_slot_per_minute*ent_edplus_price_per_slot AS cost_ent_edplus_price_per_slot,\n",
        "  CASE\n",
        "    WHEN tot_max_slot_per_minute > committed_slots THEN ((tot_max_slot_per_minute-committed_slots)*ent_edplus_price_per_slot)+(committed_slots*ent_edplus_1yc_per_slot)\n",
        "    WHEN tot_max_slot_per_minute <= committed_slots THEN (committed_slots*ent_edplus_1yc_per_slot)\n",
        "  END\n",
        "  AS cost_ent_edplus_1yc_per_slot,\n",
        "  CASE\n",
        "    WHEN tot_max_slot_per_minute > committed_slots THEN ((tot_max_slot_per_minute-committed_slots)*ent_edplus_price_per_slot)+(committed_slots*ent_edplus_3yc_per_slot)\n",
        "    WHEN tot_max_slot_per_minute <= committed_slots THEN (committed_slots*ent_edplus_3yc_per_slot)\n",
        "  END\n",
        "  AS cost_ent_edplus_3yc_per_slot\n",
        "FROM\n",
        "  concat_slots)\n",
        "SELECT\n",
        "  '{earliest_date}' as date_from,\n",
        "  '{latest_date}' as date_to,\n",
        "  {committed_slots} as committed_slots,\n",
        "  cast(avg(sum_max_slot_per_minute) as INT64) AS avg_max_slots_per_billed_minute,\n",
        "  '----------------------------------' as input_pricing,\n",
        "  {std_ed_price_per_slot} as std_ed_price_per_slot_hr,\n",
        "  {ent_ed_price_per_slot} as ent_ed_price_per_slot_hr,\n",
        "  {ent_ed_1yc_per_slot} as ent_ed_1yc_per_slot_hr,\n",
        "  {ent_ed_3yc_per_slot} as ent_ed_3yc_per_slot_her,\n",
        "  {ent_edplus_price_per_slot} as ent_edplus_price_per_slot_hr,\n",
        "  {ent_edplus_1yc_per_slot} as ent_edplus_1yc_per_slot_hr,\n",
        "  {ent_edplus_3yc_per_slot} as ent_edplus_3yc_per_slot_hr,\n",
        "  count(*) as num_mins_billed,\n",
        "  TIMESTAMP_DIFF('{latest_date}', '{earliest_date}', MINUTE) AS num_mins_total,\n",
        "  '----------------------------------' as on_demand_pricing,\n",
        "  tot_bytes_billed_TB as total_TB_ondemand,\n",
        "  round((tot_bytes_billed_TB * 6.25), 2) as cost_USD_ondemand,\n",
        "  '----------------------------------' as capacity_pricing,\n",
        "  round(sum(cost_std_ed_price_per_slot), 2) as cost_USD_std_ed_no_commmit,\n",
        "  round(sum(cost_ent_ed_price_per_slot), 2) as cost_USD_ent_ed_no_commit,\n",
        "  round(sum(cost_ent_ed_1yc_per_slot)+((TIMESTAMP_DIFF('{latest_date}', '{earliest_date}', MINUTE)-count(*))*ent_ed_1yc_per_slot*{committed_slots}), 2) as cost_USD_ent_ed_1y_commit,\n",
        "  round(sum(cost_ent_ed_3yc_per_slot)+((TIMESTAMP_DIFF('{latest_date}', '{earliest_date}', MINUTE)-count(*))*ent_ed_3yc_per_slot*{committed_slots}), 2) as cost_USD_ent_ed_3y_commit,\n",
        "  round(sum(cost_ent_edplus_price_per_slot), 2) as cost_USD_ent_edplus_no_commit,\n",
        "  round(sum(cost_ent_edplus_1yc_per_slot)+((TIMESTAMP_DIFF('{latest_date}', '{earliest_date}', MINUTE)-count(*))*ent_edplus_1yc_per_slot*{committed_slots}), 2) as cost_USD_ent_edplus_1y_commit,\n",
        "  round(sum(cost_ent_edplus_3yc_per_slot)+((TIMESTAMP_DIFF('{latest_date}', '{earliest_date}', MINUTE)-count(*))*ent_edplus_3yc_per_slot*{committed_slots}), 2) as cost_USD_ent_edplus_3y_commit\n",
        "FROM\n",
        "  price_per_min_bucket\n",
        "JOIN\n",
        "  pay_as_go_tot\n",
        "ON TRUE\n",
        "where usage_time >= '{earliest_date}'\n",
        "group by tot_bytes_billed_TB\n",
        "\"\"\"\n",
        "dfx = client.query(query).to_dataframe()\n",
        "print(dfx.transpose())\n"
      ],
      "metadata": {
        "id": "_Df_uQE89fyE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# group max_slots_per_minute per 100 slots buckets. The first bucket represents 0 > 100 slots, the second 101 > 200 slots, etc\n",
        "client = bigquery.Client(project=project_id)\n",
        "query = f\"\"\"\n",
        "WITH\n",
        "  all_buckets AS (\n",
        "  SELECT\n",
        "    GENERATE_ARRAY(0, MAX(max_slot_per_minute) + 100, 100) AS bucket\n",
        "  FROM\n",
        "    `{project_id}.{dataset_id}.{table_id}`\n",
        "  WHERE\n",
        "    usage_time >= '{earliest_date}'\n",
        "    AND project_id IS NOT NULL )\n",
        "SELECT\n",
        "  bucket,\n",
        "  COUNTIF(max_slot_per_minute <= bucket + 100 AND max_slot_per_minute > bucket) AS minutes,\n",
        "FROM\n",
        "  `{project_id}.{dataset_id}.{table_id}`,\n",
        "  UNNEST((\n",
        "    SELECT\n",
        "      *\n",
        "    FROM\n",
        "      all_buckets)) AS bucket\n",
        "WHERE\n",
        "  usage_time >= '{earliest_date}'\n",
        "  AND project_id IS NOT NULL\n",
        "GROUP BY\n",
        "  bucket\n",
        "\"\"\"\n",
        "dfx = client.query(query).to_dataframe()\n",
        "print(dfx)"
      ],
      "metadata": {
        "id": "jWd4Dn-mh4gC"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}